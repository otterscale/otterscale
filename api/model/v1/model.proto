edition = "2023";

package otterscale.model.v1;

import "api/annotations.proto";
import "api/application/v1/application.proto";
import "google/protobuf/empty.proto";
import "google/protobuf/timestamp.proto";

option go_package = "github.com/otterscale/otterscale/api/model/v1;pb";

service ModelService {
  rpc ListModels(ListModelsRequest) returns (ListModelsResponse) {
    option (otterscale.api.feature) = {
      name: "model-enabled"
    };
  };

  rpc CreateModel(CreateModelRequest) returns (Model) {
    option (otterscale.api.feature) = {
      name: "model-enabled"
    };
  };

  rpc UpdateModel(UpdateModelRequest) returns (Model) {
    option (otterscale.api.feature) = {
      name: "model-enabled"
    };
  };

  rpc DeleteModel(DeleteModelRequest) returns (google.protobuf.Empty) {
    option (otterscale.api.feature) = {
      name: "model-enabled"
    };
  };

  rpc ListModelArtifacts(ListModelArtifactsRequest) returns (ListModelArtifactsResponse) {
    option (otterscale.api.feature) = {
      name: "model-enabled"
    };
  };

  rpc CreateModelArtifact(CreateModelArtifactRequest) returns (ModelArtifact) {
    option (otterscale.api.feature) = {
      name: "model-enabled"
    };
  };

  rpc DeleteModelArtifact(DeleteModelArtifactRequest) returns (google.protobuf.Empty) {
    option (otterscale.api.feature) = {
      name: "model-enabled"
    };
  };
}

message Model {
  enum Mode {
    MODE_INTELLIGENT_INFERENCE_SCHEDULING = 0;
    MODE_PREFILL_DECODE_DISAGGREGATION = 1;  // heterogeneous parallelism
  }
  message Prefill {
    uint32 replica = 1;
    uint32 vgpumem_percentage = 11;
  }
  message Decode {
    uint32 replica = 1;  // set to 1 if mode is prefill-decode disaggregation
    uint32 tensor = 2;
    uint32 vgpumem_percentage = 11;
  }
  string id = 1;
  string name = 11;
  string namespace = 12;
  string status = 13;
  string description = 14;
  google.protobuf.Timestamp first_deployed_at = 15;
  google.protobuf.Timestamp last_deployed_at = 16;
  string chart_version = 21;
  string app_version = 22;
  Mode mode = 31;
  Prefill prefill = 32;  // disabled if mode is intelligent inference scheduling
  Decode decode = 33;
  uint32 max_model_length = 34;
  repeated otterscale.application.v1.Application.Pod pods = 41;
  bool from_persistent_volume_claim = 51;
  string persistent_volume_claim_name = 52;
}

message ListModelsRequest {
  string scope = 1;
  reserved 2;
  string namespace = 3;
}

message ListModelsResponse {
  repeated Model models = 1;
  string service_uri = 2;
}

message CreateModelRequest {
  string scope = 1;
  reserved 2;
  string namespace = 3;
  string name = 4;
  string model_name = 11;
  uint64 size_bytes = 12;
  bool from_persistent_volume_claim = 13;
  string persistent_volume_claim_name = 14;
  Model.Mode mode = 21;
  Model.Prefill prefill = 22;
  Model.Decode decode = 23;
  uint32 max_model_length = 24;
}

message UpdateModelRequest {
  string scope = 1;
  reserved 2;
  string namespace = 3;
  string name = 4;
  Model.Mode mode = 21;
  Model.Prefill prefill = 22;
  Model.Decode decode = 23;
  uint32 max_model_length = 24;
}

message DeleteModelRequest {
  string scope = 1;
  reserved 2;
  string namespace = 3;
  string name = 4;
}

message ModelArtifact {
  string name = 1;
  string namespace = 2;
  string model_name = 11;
  string phase = 21;
  int64 size = 22;
  string volume_name = 31;
  google.protobuf.Timestamp created_at = 41;
}

message ListModelArtifactsRequest {
  string scope = 1;
  reserved 2;
  string namespace = 3;
}

message ListModelArtifactsResponse {
  repeated ModelArtifact model_artifacts = 1;
}

message CreateModelArtifactRequest {
  string scope = 1;
  reserved 2;
  string namespace = 3;
  string name = 4;
  string model_name = 11;
  int64 size = 12;
}

message DeleteModelArtifactRequest {
  string scope = 1;
  reserved 2;
  string namespace = 3;
  string name = 4;
}
